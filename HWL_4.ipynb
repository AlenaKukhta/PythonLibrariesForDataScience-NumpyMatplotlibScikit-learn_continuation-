{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 4. Оценка и интерпретация полученной модели. Обсуждение курсового проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация:\n",
    "- минимизирует функционал ошибки с ограничением весов;\n",
    "- \"штрафует\" модель за слишком большие веса путем добавления нового члена к ошибке:\n",
    "$$Q(w, X) + \\lambda ||w||^{2} \\rightarrow \\underset{w}{\\text{min}};$$\n",
    "- позволяет контролировать рост модели опираясь на анализ ее сложности;\n",
    "- помогает решить проблему переобученности модели;\n",
    "- работает либо по принципу раннего останова либо по принципу упрощения дерева (pruning).\n",
    "\n",
    "Параметр $\\lambda$ позволяет управлять степенью строгости штрафа (подбираем по кросс-валидации):\n",
    "\n",
    "- чем меньше значение $\\lambda$, тем больше штраф за признак с высоким весом. В процессе такого обучения получается **большая** сложность модели. Если уменьшать значение $\\lambda$, в какой-то момент появляется вероятность чрезмерного усложнения модели и переобучения;\n",
    "\n",
    "- чем больше значение $\\lambda$, тем меньше штраф за признак с высоким весом. В процессе такого обучения получается **меньшая** сложность модели. Если увеличивать значение $\\lambda$, в какой-то момент оптимальным для модели окажется зануление всех весов.\n",
    "\n",
    "\n",
    "При подходе раннего останова штрафуются, например:\n",
    "\n",
    "- max_depth - максимальная глубина дерева; \n",
    "\n",
    "- min_samples_leaf - минимальное число объектов в листе. Например, если он равен 5, то дерево будет порождать только те классифицирующие правла, которые верны как мининмум для 5 объектов. Можно задать числом или процентом от общего числа объектов (по дефолту — 1);\n",
    "\n",
    "- min_samples_split - минимальное количество объектов, необходимое для разделения внутреннего узла. Можно задать числом или процентом от общего числа объектов (по дефолту — 2);\n",
    "\n",
    "- max_features - максимальное чило признаков, по которым ищется лучшее разбиение в дереве (необходимо из-за того, что при большом количестве признаков будет \"дорого\" искать лучшее (по критерию типа прироста информации) разбиение среди всех признаков).\n",
    "\n",
    "При подходе упрощения дерева (pruning: дерево сначала полностью строится, затем уменьшается до оптимального с точки зрения достижения максимальной обучающей способности размера путем объединения некоторых концевых вершин) штрафуются, например, при применении:\n",
    "\n",
    "а) XGBoost: importance_type\n",
    "$$Gain - \\gamma \\geqslant 0 \\Rightarrow оставляем$$\n",
    "$$Gain - \\gamma < 0 \\Rightarrow убираем;$$\n",
    "\n",
    "б) LightGBM: max_depth;\n",
    "\n",
    "в) CatBoost: max_ctr_complexity, store_all_simple_ctr;\n",
    "\n",
    "г) Cost Complexity Pruning- Alpha: ccp_alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом шаге построения дерева выбирается признак и его значение, которые дают наилучшее разбиение данных. В зависимости от алгоритма, применяются разные способы подсчета прироста информации, чтобы вычислить наилучшее разбиение(например считают критерий Джини или энтропию в каждой части разбиения). Важность признака в дереве определяется степенью снижения меры неопределенности, т.е. как раз критерия Джини или энтропии и других. Чем выше признак в дереве, тем он важнее. В ансамблях деревьев важность признака считается как среднее арифметическое по всем важностям данного признака для каждого дерева. В случае, если признак не присутствует в дереве, его важность считается нулевой.\n",
    "\n",
    "Методами вычисления важности признаков в алгоритмах ансамблей деревьев являются:\n",
    "\n",
    "- MDI (Mean Decrease in Impurity, Gini Importance). Чем больше, тем выше важность. Данный метод уменьшает количество значений признака и вычисляет точность модели после изменений. Чем сильнее уменьшение точности модели, тем выше важность признака. Может некорректно отображать важность, если признак содержит большое количество уникальных значений;\n",
    "\n",
    "- MDA (Mean Decrease in Accuracy, Permutation Importance). Данный метод перемешивает значения в признаке и измеряет точность модели. В отличие от MDI не имеет проблем с признаками с большим количеством уникальных значений;\n",
    "\n",
    "- Boruta. Данный метод так же как MDA перемешивает значения в признаках, но перемешанные данные добавляются в основной датасет."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
